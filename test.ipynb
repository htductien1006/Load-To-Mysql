{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ductien/spark-3.3.2-bin-hadoop3')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/29 01:11:21 WARN Utils: Your hostname, DT-Kubuntu resolves to a loopback address: 127.0.1.1; using 192.168.1.7 instead (on interface wlp3s0)\n",
      "23/04/29 01:11:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/04/29 01:11:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jars\", \"./mysql-connector-j-8.0.32.jar\")\\\n",
    "                    .appName(\"MySQL_loader\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mark_2017_df = spark.read.csv(\"./CleanedDatasets/Mark2017\", header= True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mark_18_22_df = spark.read.csv(\"./CleanedDatasets/Mark_2018_2022\", header= True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "benchmark_df = spark.read.csv(\"./CleanedDatasets/cleaned_uni_mark\", header= True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Spark dataframe to MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "dbName = os.getenv(\"DB_NAME\")\n",
    "dbUser = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "port = os.getenv(\"DB_PORT\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load to province table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_2017_df.createTempView(\"PROVINCE\")\n",
    "provinceDF = spark.sql(\"SELECT DISTINCT province_code, province FROM PROVINCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- province_code: integer (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "provinceDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF = provinceDF.sort(\"province_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provinceDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "newRow = spark.createDataFrame([(39,'phuyen')], provinceDF.columns)\n",
    "provinceDF = provinceDF.union(newRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "newRow = spark.createDataFrame([(45,'ninhthuan')], provinceDF.columns)\n",
    "provinceDF = provinceDF.union(newRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF = provinceDF.sort('province_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF = provinceDF.withColumnRenamed('province', 'ProvinceName') \\\n",
    "                        .withColumnRenamed('province_code', 'provinceCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- provinceCode: long (nullable = true)\n",
      " |-- ProvinceName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "provinceDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provinceDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinceDF.write.format('jdbc').options(\n",
    "    url = 'jdbc:mysql://localhost:3306/equivalent_score',\n",
    "    driver = 'com.mysql.jdbc.Driver',\n",
    "    dbtable = \"DimProvince\",\n",
    "    user = dbUser,\n",
    "    password = password\n",
    ").mode('append').save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load to Year table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearDF = spark.createDataFrame([\n",
    "    Row(year = 2018),\n",
    "    Row(year = 2019),\n",
    "    Row(year = 2020),\n",
    "    Row(year = 2021),\n",
    "    Row(year = 2022)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2018|\n",
      "|2019|\n",
      "|2020|\n",
      "|2021|\n",
      "|2022|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yearDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearDF.write.format('jdbc').options(\n",
    "    url = 'jdbc:mysql://localhost:3306/equivalent_score',\n",
    "    driver = 'com.mysql.jdbc.Driver',\n",
    "    dbtable = \"DimYear\",\n",
    "    user = dbUser,\n",
    "    password = password\n",
    ").mode('append').save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load to DimUniverity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_df.createTempView('UNI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uni_code: string (nullable = true)\n",
      " |-- uni_name: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- major_code: string (nullable = true)\n",
      " |-- major_name: string (nullable = true)\n",
      " |-- benchmark: double (nullable = true)\n",
      " |-- subject_group: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniDF = spark.sql(\"SELECT uni_code AS UniCode, uni_name AS UniName, \\\n",
    "                   major_code AS MajorCode, major_name AS MajorName, \\\n",
    "                   subject_group AS SubjectGroup, benchmark AS BenchMark, year AS Year FROM UNI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when, isnan, isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+---------+------------+---------+----+\n",
      "|UniCode|UniName|MajorCode|MajorName|SubjectGroup|BenchMark|Year|\n",
      "+-------+-------+---------+---------+------------+---------+----+\n",
      "|      0|      0|       37|        0|           0|        0|   0|\n",
      "+-------+-------+---------+---------+------------+---------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "uniDF.select([count(when(isnan(c) | isnull(c), c)).alias(c) for c in uniDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniDF = uniDF.dropna(subset= uniDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+---------+------------+---------+----+\n",
      "|UniCode|UniName|MajorCode|MajorName|SubjectGroup|BenchMark|Year|\n",
      "+-------+-------+---------+---------+------------+---------+----+\n",
      "|      0|      0|        0|        0|           0|        0|   0|\n",
      "+-------+-------+---------+---------+------------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uniDF.select([count(when(isnan(c) | isnull(c), c)).alias(c) for c in uniDF.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniDF = uniDF.filter(col('BenchMark') <= 32.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|max(length(major_code))|\n",
      "+-----------------------+\n",
      "|                     25|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT max(length(major_code)) FROM UNI').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34734"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "uniDF.write.format('jdbc').options(\n",
    "    url = 'jdbc:mysql://localhost:3306/equivalent_score',\n",
    "    driver = 'com.mysql.jdbc.Driver',\n",
    "    dbtable = \"University\",\n",
    "    user = dbUser,\n",
    "    password = password\n",
    ").mode('append').save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load to FactScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_18_22_df.createTempView('student1822')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- student_id: integer (nullable = true)\n",
      " |-- mathematics: double (nullable = true)\n",
      " |-- literature: double (nullable = true)\n",
      " |-- english: double (nullable = true)\n",
      " |-- physics: double (nullable = true)\n",
      " |-- chemistry: double (nullable = true)\n",
      " |-- biology: double (nullable = true)\n",
      " |-- history: double (nullable = true)\n",
      " |-- geography: double (nullable = true)\n",
      " |-- civic_education: double (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- province_code: integer (nullable = true)\n",
      " |-- combined_natural_sciences: double (nullable = true)\n",
      " |-- combined_social_sciences: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mark_18_22_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1822 = spark.sql(\"SELECT student_id as studentID, literature, mathematics as math, \\\n",
    "                         english, physics, chemistry, biology, history, geography, \\\n",
    "                         civic_education as civil, province_code as ProvinceCode, Year as year FROM student1822\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1822 = student1822.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student1822.select('ProvinceCode').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctProv = spark.sql(\"SELECT DISTINCT province_code FROM student1822\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinctProv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3815783"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student1822.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===================>                                    (6 + 11) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----+-------+-------+---------+-------+-------+---------+-------+------------+----+\n",
      "|studentID|literature|math|english|physics|chemistry|biology|history|geography|  civil|ProvinceCode|year|\n",
      "+---------+----------+----+-------+-------+---------+-------+-------+---------+-------+------------+----+\n",
      "|        0|         0|   0|      0|2327577|  2327932|2329854|1450340|  1452611|1459756|           0|   0|\n",
      "+---------+----------+----+-------+-------+---------+-------+-------+---------+-------+------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "student1822.select([count(when(isnan(c) | isnull(c), c)).alias(c) for c in student1822.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:==========>                                             (3 + 13) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|max(literature)|\n",
      "+---------------+\n",
      "|           10.0|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "student1822.select(max(col('literature'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1822.write.format('jdbc').options(\n",
    "    url = 'jdbc:mysql://localhost:3306/equivalent_score',\n",
    "    driver = 'com.mysql.jdbc.Driver',\n",
    "    dbtable = \"FactScore\",\n",
    "    user = dbUser,\n",
    "    password = password\n",
    ").partitionBy('year').mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
